---
title: "Project 3"
authors: "Sam Reeves, Henry Owens, Claire Meyer, Michael Ippolito"
date: "3/23/2021"
output:
  html_document:
    df_print: paged
    toc: false
    toc_float: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE}
library(RCurl)
library(httr)
library(jsonlite)
library(tidyverse)
library(tokenizers)
library(stopwords)
library(kableExtra)
```

## Getting the skills data
Run function to collect results and construct query strings, build a DF of 100 results.

```{r dataset}

# See if we've already downloaded the skills; if so, just load the csv
if (file.exists("search_results.csv")) {

  # Load from csv
  base_df <- read.csv("search_results.csv")

} else {
  # Pull first 10 results
  base_url = "https://www.googleapis.com/customsearch/v1?key=AIzaSyDGGlvHG0qLpOjLF5_k-ojZtPKjyFOSUYM&cx=1f1bbcd0558947d90&q='data+scientist+skills'"
  base_results <- fromJSON(txt=base_url)
  
  # Create data frame, discarding the 11th column, which we don't need and is problematic
  # because it's a nested list.
  base_df <- base_results$items[, -11]
  
  # Create a query base to iterate on
  query_base <- "https://www.googleapis.com/customsearch/v1?key=AIzaSyDGGlvHG0qLpOjLF5_k-ojZtPKjyFOSUYM&cx=1f1bbcd0558947d90&exactTerms='data+science'&q='data+scientist+skills'&start=" 
  
  # Created a search results function that inputs query string 
  # and outputs results that can be bound to base_df
  get_search_results <- function(q_string) {
    results <- fromJSON(txt = q_string)
    items_df <- results$items[, -11]
    return(items_df)
  }
  
  # Run a for loop to generate the needed queries for the next 90 results.
  x <- c(1:9)
  for (i in x) {
    query <- paste(query_base, (i * 10 + 1), sep="")
    items_df <- get_search_results(query)
    base_df <- rbind(base_df, items_df)
  }

  # In case of rate limiting, write to CSV
  write.csv(base_df,'search_results.csv',row.names = TRUE)
}

```

## Create stopword list

```{r stopwords}

# Tokenize snippets
# https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html

# Fetch stopword list
stopwordlist <- stopwords::stopwords("en")

# Remove bare numbers
stopwordlist <- append(stopwordlist, sapply(c(1:100), "as.character"))

# Remove months and years
stopwordlist <- append(stopwordlist, c("2020", "2021", "2019", "2018", "2017", tolower(month.abb[c(1:12)])))

# Remove other miscellaneous words
stopwordlist <- append(stopwordlist, c("15,000", "11000"))

```

## Create skills corpus

Combine result snippets into a corpus.

```{r corpus}

# Create lowercase corpus of skills
skills_corpus_lower <- tolower(paste(base_df$snippet, collapse = ""))

# Remove stop words from the corpus
for (stopword in stopwordlist) {
  replaceRegex = paste("\\b", stopword, "\\b", sep = "")
  skills_corpus_lower <- str_replace_all(skills_corpus_lower, replaceRegex, "")
}

# Remove non alphabetical characters (except apostrophes) from the corpus,
# then remove any duplicate spaces it may have created as a result
skills_corpus_lower <- str_replace_all(skills_corpus_lower, "[^a-z']", " ")
skills_corpus_lower <- str_replace_all(skills_corpus_lower, " +", " ")

# Tokenize using n-grams between 1 and 3 words long
skills_tokens <- tokenize_ngrams(skills_corpus_lower, n = 3, n_min = 1, stopwords = stopwordlist)[[1]]
skills_tokens <- as.list(tokens)

```

## CUNY scrape

Scrape the CUNY master's in data science course catalog.

```{r cuny_scrape}

# See if we've already done this (i.e. see if a csv file exists in the working directory)
if (file.exists("cuny.csv")) {

  results <- read.csv("cuny.csv")

} else {

  # Read CUNY master's in data science course catalog
  cunyUrl <- "http://catalog.sps.cuny.edu/preview_program.php?catoid=2&poid=607"
  catalog <- getURL(cunyUrl)

  # Each course will show up like this in the html source:
  # <a href="#" aria-expanded="false" onClick="showCourse('2', '979',this, 'a:2:{s:8:~location~;s:7:~program~;s:4:~core~;s:4:~1598~;}'); return false;">DATA 602 - Advanced Programming Techniques (3 Credits) </a>
  # Use regex to parse
  matches <- str_match_all(catalog, "showCourse\\('(\\d+)', *'(\\d+)',this, '(.+?)'.+?>(DATA \\d{3}) - (.+?) \\(?(\\d+.*?) Credits\\)?.+?<\\/a>")
  
  # Build URL to fetch each course
  # We need to end up with this format:
  # http://catalog.sps.cuny.edu/ajax/preview_course.php?catoid=2&coid=979&link_text=&display_options=a:2:{s:8:~location~;s:7:~program~;s:4:~core~;s:4:~1598~;}&show
  courseUrls <- str_c("http://catalog.sps.cuny.edu/ajax/preview_course.php?catoid=",
        matches[[1]][,2], "&coid=", matches[[1]][,3], "&link_text=&display_options=", matches[[1]][,4], "&show")
  
  # Build dataframe
  dfcuny <- data.frame(courseno = matches[[1]][,5], descr = matches[[1]][,6], credits = matches[[1]][,7], url = courseUrls)

  # Fetch course descriptions using courseUrls
  #results <- sapply(dfcuny$url, function(x) try(getURL(x)))

  # Extract first element from results
  tmpa <- sapply(results, "[", 1)

  # Parse results; this is the part we're interested in:
  # DATA 607</a><span style="display: none !important">&#160;</span></em><br>In this course students will learn...<br><br>
  coursematches <- str_match_all(tmpa, regex('<\\/em>.*<br>(.+?)<br><br>', dotall = TRUE))

  # Extract second element from results
  tmpb <- sapply(coursematches, "[", 2)

  # Add the course text to the data frame
  dfcuny <- mutate(dfcuny, course_text = tmpb)

  # write to CSV
  write.csv(dfcuny,'cuny.csv',row.names = TRUE)
}

```

## CUNY corpus

Combine result snippets into a corpus.

```{r cuny_corpus}

# Create cuny corpus
cuny_corpus_lower <- tolower(paste(dfcuny$course_text, collapse = ""))

# Remove stop words from the corpus
for (stopword in stopwordlist) {
  replaceRegex = paste("\\b", stopword, "\\b", sep = "")
  cuny_corpus_lower <- str_replace_all(cuny_corpus_lower, replaceRegex, "")
}

# Remove non alphabetical characters (except apostrophes) from the corpus,
# then remove any duplicate spaces it may have created as a result
cuny_corpus_lower <- str_replace_all(cuny_corpus_lower, "[^a-z']", " ")
cuny_corpus_lower <- str_replace_all(cuny_corpus_lower, " +", " ")

# Tokenize using n-grams between 1 and 3 words long
cuny_tokens <- tokenize_ngrams(cuny_corpus_lower, n = 3, n_min = 1, stopwords = stopwordlist)[[1]]
cuny_tokens <- as.list(tokens)

```

## Count tokens

Count the number of occurences of each token in the skills corpus and in thew CUNY course catalog corpus.

```{r counts}

# Combine the skills and cuny token lists and remove duplicates
all_tokens <- c(skills_tokens, cuny_tokens)
all_tokens <- unique(all_tokens)

# Create data frame to hold counts
count_table <- data.frame(c(1:length(all_tokens)), 0, 0)
colnames(count_table) <- c("token", "skills_count", "cuny_count")

# Walk each token and count the number of occurrences in each corpus
for (i in c(1:length(all_tokens))) {
  
  # Add an entry in the counts table for this token
  count_table$token[i] <- as.character(all_tokens[i])

  # Build regex based on this token occuring along word boundaries
  tokenRegex <- paste("\\b", as.character(all_tokens[i]), "\\b", sep = "")
  
  # Count the number of occurences of this token in the skills corpus
  count_table$skills_count[i] <- str_count(skills_corpus_lower, pattern = tokenRegex)

  # Count the number of occurences of this token in the cuny corpus
  count_table$cuny_count[i] <- str_count(cuny_corpus_lower, pattern = tokenRegex)
}

# Add a % of snippets field
count_table <- count_table %>% 
  mutate(pct_skills_snippets = skills_count / 100) %>%
  mutate(pct_cuny_courses = cuny_count / 19)

# write to CSV
write.csv(count_table, 'counts.csv', row.names = TRUE)

```
